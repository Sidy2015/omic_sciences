---
title: "Singular Value Decomposition"
layout: page
---

```{r options, echo=FALSE}
library(knitr)
```
#Dimension Reduction: SVD
Matrix data have some special statistical methods that can be applied to them. One category of statistical dimension reduction techniques is commonly called principal components analysis (PCA) or the singular value decomposition (SVD). These techniques generally are applied in situations where the rows of a matrix represent observations of some sort and the columns of the matrix represent features or variables (but this is by no means a requirement).

In an abstract sense, the SVD or PCA can be thought of as a way to approximate a high-dimensional matrix (i.e. a large number of columns) with a a few low-dimensional matrices. So there’s a bit of data compression angle to it. We’ll take a look at what’s going on in this chapter.

```{r}
set.seed(12345)
dataMatrix <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
heatmap(dataMatrix)
```

Not surprisingly, there aren’t really any interesting patterns given that we just simulated random noise. At least it’s good to know that the clustering algorithm won’t pick up something when there’s nothing there!

But now what if there were a pattern in the data? How would we discover it?

Let’s first simulate some data that indeed does have a pattern. In the code below, we cycle through all the rows of the matrix and randomly add 3 to the last 5 columns of the matrix.
```{r}
set.seed(678910)
for (i in 1:40) {
coinFlip <- rbinom(1, size = 1, prob = 0.5)
## If coin is heads add a common pattern to that row
 if (coinFlip) {
 dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 3), each = 5)
 }
}

image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
```
You can see that some of the rows on the right side of the matrix have higher values than on the left side.

Now what happens if we cluster the data?

```{r}
heatmap(dataMatrix)
```
##Patterns in rows and columns
In general, with matrix data, there may be patterns that occur accross the rows and columns of the matrix. In the example above, we shifted the mean of some of the observations in columns 5 through 10. We can display this a bit more explicitly by looking at the row and column means of the data.

```{r}
 library(dplyr)
 hh <- dist(dataMatrix) %>% hclust
 dataMatrixOrdered <- dataMatrix[hh$order, ]
 par(mfrow = c(1, 3))
 
 ## Complete data
 image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
 
 ## Show the row means
 plot(rowMeans(dataMatrixOrdered), 40:1, , xlab = "Row Mean", ylab = "Row", pch = 19)
 
 ## Show the column means
 plot(colMeans(dataMatrixOrdered), xlab = "Column", ylab = "Column Mean", pch = 19)
```
However, there may be other patterns beyond a simple mean shift and so more sophisticated methods will be needed. Futhermore, there may be multiple patterns layered on top of each other so we need a method that can distangle these patterns.

##Related problem
Here’s another way to formulate the problem that matrix data present. Suppose you have multivariate observations

$$X_1,...,X_n$$

so that each of the n observations has m features,


$$X_1 = (X_{11},...,X_{1m})$$

X
1
,
…
,
X
n
 

Given this setup, the goal is to find a new set of variables/features that are uncorrelated and explain as much variance in the data as possible. Put another way, if you were to put all these multivariate observations together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data.

The first goal is statistical in nature and the second goal is perhaps better characterized as lossy data compression.

##SVD and PCA

If X is a matrix with each variable in a column and each observation in a row then the SVD is a matrix decomposition that represents X as a matrix product of three matrices:

$$X = UDV^-1$$

where the columns of U (left singular vectors) are orthogonal, the columns of V (right singular vectors) are orthogonal and  
D is a diagonal matrix of singular values.

Principal components analysis (PCA) is simply an application of the SVD. The principal components are equal to the right singular values if you first scale the data by subtracting the column mean and dividing each column by its standard deviation (that can be done with the scale() function).

##Unpacking the SVD: u and v

The SVD can be computed in R using the svd() function. 

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
```

Here, we scale our original matrix data with the pattern in it and apply the svd.
The svd() function returns a list containing three components named u, d, and v. The u and v components correspond to the matrices of left and right singular vectors, respectively, while the d component is a vector of singular values, corresponding to the diagonal of the matrix D described above.

Below we plot the first left and right singular vectors along with the original data.
```{r}
 par(mfrow = c(1, 3))
 image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1], main = "Original Data")
 plot(svd1$u[, 1], 40:1, ylab = "Row", xlab = "First left singular vector", pch = 19)
 plot(svd1$v[, 1], xlab = "Column", ylab = "First right singular vector", pch = 19)
```

You can see how the first left and right singular vectors pick up the mean shift in both the rows and columns of the matrix.

##SVD for data compression
If we believed that the first left and right singular vectors, call them u1 and v1, captured all of the variation in the data, then we could approximate the original data matrix with

$$X \approx u_1v^t_1$$

Thus, we would reduce 400 numbers in the original matrix to 40 + 10 = 50 numbers in the compressed matrix, a nearly 90% reduction in information. Here’s what the original data and the approximation would look like.

## Approximate original data with outer product of first singular vectors
```{r}
 approx <- with(svd1, outer(u[, 1], v[, 1]))
#approx = svd1$u[,1] %*%t(svd1$v[,1])
 ## Plot original data and approximated data
 par(mfrow = c(1, 2))
 image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1], main = "Original Matrix")
 image(t(approx)[, nrow(approx):1], main = "Approximated Matrix")
```

Obviously, the two matrices are not identical, but the approximation seems reasonable in this case. This is not surprising given that there was only one real feature in the original data.

## Components of the SVD - Variance explained
The statistical interpretation of singular values is in the form of variance in the data explained by the various components. The singular values produced by the svd() are in order from largest to smallest and when squared are proportional the amount of variance explained by a given singular vector.

To show how this works, here’s a very simple example. First, we’ll simulate a “dataset” that just takes two values, 0 and 1.
```{r}
constantMatrix <- dataMatrixOrdered * 0
for (i in 1:dim(dataMatrixOrdered)[1]) {
    constantMatrix[i, ] <- rep(c(0, 1), each = 5)
}
```
Then we can take the SVD of this matrix and show the singular values as well as the proportion of variance explained.
```{r}
 svd1 <- svd(constantMatrix)
 par(mfrow = c(1, 3))
 image(t(constantMatrix)[, nrow(constantMatrix):1], main = "Original Data")
 plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
 plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch = 19)
```


As we can see from the right-most plot, 100% of the variation in this “dataset” can be explained by the first singular value. Or, all of the variation in this dataset occurs in a single dimension. This is clear because all of the variation in the data occurs as you go from left to right across the columns. Otherwise, the values of the data are constant.

In the plot below, we plot the singular values (left) and the proportion of variance explained for the slightly more complex dataset that we’d been using previously.
```{r}
 par(mfrow = c(1, 2))
 svd1 <- svd(scale(dataMatrixOrdered))
 plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
 plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch = 19)
```
We can see that the first component explains about 40% of all the variation in the data. In other words, even though there are 10 dimensions in the data, 40% of the variation in the data can be explained by a single dimension. That suggests that the data could be simplified quite a bit, a phenomenon we observed in the last section where it appeared the data could be reasonably approximated by the first left and right singular vectors.

##
##
## Singular Value Decomposition

In a previous section, we motivated dimension reduction and showed a transformation that permitted us to approximate the distance between two dimensional points with just one dimension. The singular value decomposition (SVD) is a generalization of the algorithm we used in the motivational section. Lets look at a more complex example.

The main result SVD provides is that we can write an $m \times n$, matrix $\mathbf{Y}$ as

$$\mathbf{U}^\top\mathbf{Y} = \mathbf{DV}^\top$$

With:

* $\mathbf{U}$ is an $m \times p$ orthogonal matrix
* $\mathbf{D}$ is an $p \times p$ diagonal matrix
* $\mathbf{V}$ is an $n \times p$ orthogonal matrix 

with $p=\mbox{min}(m,n)$. $\mathbf{U}^\top$ provides a rotation of our data $\mathbf{Y}$ that turns out to be very useful because the variability (sum of squares to be precise) of the columns of $\mathbf{U}^\top \mathbf{Y}=\mathbf{VD}$ are decreasing.
Because $\mathbf{U}$ is orthogonal, we can write the SVD like this: 

$$\mathbf{Y} = \mathbf{UDV}^\top$$

In fact, this formula is much more commonly used. We can also write the transformation like this:

$$\mathbf{YV} = \mathbf{UD}$$

This transformation of $Y$ also results in a matrix with column of decreasing sum of squares.


Applying the SVD to the motivating example we have:

```{r simulate twin heights,message=FALSE}
library(rafalib)
library(MASS)
n <- 100
y <- t(mvrnorm(n,c(0,0), matrix(c(1,0.95,0.95,1),2,2)))
s <- svd(y)
```

We can immediately see that applying the SVD results in a transformation very similar to the one we used in the motivating example:
```{r}
round(sqrt(2) * s$u , 3)
```

The plot we showed after the rotation was showing what we call the _principal components_: the second plotted against the first. To obtain the principal components from the SVD, we simply need the columns of the rotation $\mathbf{U}^\top\mathbf{Y}$ :

```{r PCAplot, fig.cap="Second PC plotted against first PC for the twins height data."}
PC1 = s$d[1]*s$v[,1]
PC2 = s$d[2]*s$v[,2]
plot(PC1,PC2,xlim=c(-3,3),ylim=c(-3,3))
```


#### How is this useful?

It is not immediately obvious how incredibly useful the SVD can be, so let's consider some examples. In this example, we will greatly reduce the dimension of $V$ and still be able to reconstruct $Y$.

Let's compute the SVD on the gene expression table we have been working with. We will take a subset of 100 genes so that computations are faster.
```{r}
library(tissuesGeneExpression)
data(tissuesGeneExpression)
set.seed(1)
ind <- sample(nrow(e),500) 
Y <- t(apply(e[ind,],1,scale)) #standardize data for illustration
```

The `svd` command returns the three matrices (only the diagonal entries are returned for $D$)
```{r}
s <- svd(Y)
U <- s$u
V <- s$v
D <- diag(s$d) ##turn it into a matrix
```

First note that we can in fact reconstruct y:

```{r}
Yhat <- U %*% D %*% t(V)
resid <- Y - Yhat
max(abs(resid))
```

If we look at the sum of squares of $\mathbf{UD}$, we see that the last few are quite close to 0 (perhaps we have some replicated columns).  

```{r D_entries, fig.cap="Entries of the diagonal of D for gene expression data."}
plot(s$d)
```

This implies that the last columns of `V` have a very small effect on the reconstruction of `Y`. To see this, consider the extreme example in which the last entry of $V$ is 0. In this case the last column of $V$ is not needed at all. Because of the way the SVD is created, the columns of $V$ have less and less influence on the reconstruction of $Y$. You commonly see this described as "explaining less variance". This implies that for a large matrix, by the time you get to the last columns, it is possible that there is not much left to "explain" As an example, we will look at what happens if we remove the four last columns:

```{r}
k <- ncol(U)-4
Yhat <- U[,1:k] %*% D[1:k,1:k] %*% t(V[,1:k])
resid <- Y - Yhat 
max(abs(resid))
```

The largest residual is practically 0, meaning that we `Yhat` is practically the same as `Y`, yet we need 4 fewer dimensions to transmit the information.

By looking at $d$, we can see that, in this particular dataset, we can obtain a good approximation keeping only 94 columns. The following plots are useful for seeing how much of the variability is explained by each column:

```{r percent_var_explained, fig.cap="Percent variance explained by each principal component of gene expression data."}
plot(s$d^2/sum(s$d^2)*100,ylab="Percent variability explained")
```

We can also make a cumulative plot:

```{r cum_variance_explained, fig.cap="Cumulative variance explained by principal components of gene expression data."}
plot(cumsum(s$d^2)/sum(s$d^2)*100,ylab="Percent variability explained",ylim=c(0,100),type="l")
```

Although we start with 189 dimensions, we can approximate $Y$ with just 95:

```{r reconstruction_with_less_dimensions, fig.cap="Residuals from comparing a reconstructed gene expression table using 95 PCs to the original data with 189 dimensions."}
k <- 95 ##out a possible 189
Yhat <- U[,1:k] %*% D[1:k,1:k] %*% t(V[,1:k])
resid <- Y - Yhat
boxplot(resid,ylim=quantile(Y,c(0.01,0.99)),range=0)
```

Therefore, by using only half as many dimensions, we retain most of the variability in our data:

```{r}
var(as.vector(resid))/var(as.vector(Y))
```

We say that we explain 96% of the variability.

Note that we can compute this proportion from $D$:
```{r}
1-sum(s$d[1:k]^2)/sum(s$d^2)
```
The entries of $D$ therefore tell us how much each PC contributes in term of variability explained.

#### Highly correlated data

To help understand how the SVD works, we construct a dataset with two highly correlated columns. 

For example:

```{r}
m <- 100
n <- 2
x <- rnorm(m)
e <- rnorm(n*m,0,0.01)
Y <- cbind(x,x)+e
cor(Y)
```
In this case, the second column adds very little "information" since all the entries of `Y[,1]-Y[,2]` are close to 0. Reporting `rowMeans(Y)` is even more efficient since `Y[,1]-rowMeans(Y)` and `Y[,2]-rowMeans(Y)` are even closer to 0. `rowMeans(Y)`  turns out to be the information represented in the first column on $U$. The SVD helps us notice that we explain almost all the variability with just this first column:

```{r}
d <- svd(Y)$d
d[1]^2/sum(d^2)
```

In cases with many correlated columns, we can achieve great dimension reduction:

```{r}
m <- 100
n <- 25
x <- rnorm(m)
e <- rnorm(n*m,0,0.01)
Y <- replicate(n,x)+e
d <- svd(Y)$d
d[1]^2/sum(d^2)
```
88888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888






